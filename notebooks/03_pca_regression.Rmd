---
title: "PCA Regression"
author: "Catalina Valencia"
output: html_document
---

**Project:** US Crime Rate Modeling: Comparing Regression, Regularization, and Tree-Based Methods

**File:** 04_pca_regression.R

### Objectives:
- Apply Principal Component Analysis (PCA) to the US crime dataset.
- Construct a regression model using a reduced set of principal components.
- Express the fitted model in terms of the original predictor variables.

### Notes:
- PCA is performed on scaled predictors using prcomp(scale. = TRUE).
- No observations are removed from the dataset.
- Model quality is assessed using both in-sample fit and cross-validation.

Environment Setup

```{r}
rm(list = ls())
set.seed(1)
```

Load Data
```{r}
crimedata <- read.table(
  "../Data/uscrime.txt",
  header = TRUE,
  stringsAsFactors = FALSE
)
```

Separate response and predictors

```{r}
y <- crimedata$Crime
X <- crimedata[, names(crimedata) != "Crime"]
```

### Principal Component Analysis

- Reduce predictor dimensionality.

- Mitigate multicollinearity among variables.

- Preserve the majority of variability in the original data.

```{r}
pca_fit <- prcomp(X, scale. = TRUE)
summary(pca_fit)
```

**Results:**

- Total number of predictors: 15

- Proportion of variance explained by leading components:

     PC1  = 0.4013
     PC2  = 0.1868
     PC3  = 0.1337
     PC4  = 0.0775
     PC5  = 0.0639

- Cumulative variance:
     PC1–PC3 = 0.7217
     PC1–PC5 = 0.8631

The first k principal components are retained to balance variance preservation and model complexity.


**Selection of Number of Components**

- Retain enough components to explain a large fraction of total variance.

- Avoid overfitting given the small sample size (n = 47).

The first five principal components explain approximately 86.31% of the total variance in the predictor space.

```{r}
k <- 5

Z <- pca_fit$x[, 1:k]
```

### Regression on Principal Components

- Fit a linear regression using the retained principal components.

- Evaluate in-sample model fit.

```{r}
pcr_model <- lm(y ~ Z)
summary(pcr_model)
```

**Results:**

- Number of principal components used: k = 5

- Residual standard error: 244.0000

- Multiple R-squared: 0.6452

- Adjusted R-squared: 0.6019

Compared to the full linear regression model:

- Full model R-squared: 0.8030

- Full model adjusted R-squared: 0.7080

The PCR model explains approximately 64.52% of the variability in the response using five orthogonal components derived from the original predictors. While the in-sample fit is lower than that of the full linear model, the reduction in dimensionality mitigates multicollinearity and reduces variance inflation.

This trade-off reflects the bias–variance balance inherent in principal component regression, favoring model stability and interpretability over maximal in-sample explanatory power.

**Expressing the PCR Model in Original Variables**

- Translate the PCR coefficients back into the original predictor space.

- Enable interpretation and prediction on new observations.

Extract principal component coefficients (excluding intercept)

```{r}
beta_pc <- coef(pcr_model)[-1]
```

Map coefficients back to standardized original variables

```{r}
beta_std <- pca_fit$rotation[, 1:k] %*% beta_pc
```

Unscale coefficients to original variable units

```{r}
beta_original <- beta_std / pca_fit$scale
```

Recover intercept in original scale

```{r}
intercept_original <- coef(pcr_model)[1] -
  sum(beta_original * pca_fit$center)

beta_original
intercept_original
```

**Results:**

- Intercept (original scale): -5933.8370

- Largest-magnitude coefficients (original variables):
     LF    : 1886.9460
     Prob  : -1523.5210

Additional notable coefficients:
     U1    : 159.0115
     So    : 79.0192
     M     : 48.3737

These coefficients are obtained by back-transforming the principal component regression estimates to the original predictor space. As a result, they reflect combined effects of correlated predictors captured by the retained principal components, rather than isolated marginal effects as in standard multiple linear regression.

Coefficient magnitudes should therefore be interpreted cautiously and primarily in terms of relative influence, not direct causal impact.

### Cross-Validation for Model Quality

- Evaluate predictive performance under resampling.

- Compare PCR generalization to baseline regression models.

```{r}
library(DAAG)
```

Cross-validation using reconstructed PCR model

```{r}
pcr_lm <- lm(
  Crime ~ .,
  data = data.frame(Crime = y, X)
)

cv_pcr <- cv.lm(crimedata, pcr_lm, m = 5)
```

Compute cross-validated R-squared

```{r}
SStot <- sum((y - mean(y))^2)
SSres_cv <- attr(cv_pcr, "ms") * nrow(crimedata)

cv_r2_pcr <- 1 - SSres_cv / SStot
cv_r2_pcr
```

**Results:**

- Cross-validated R-squared (PCR): 0.4198

Baseline comparison:

- Full linear model CV R²: ≈ 0.4100

- Reduced linear model CV R²: ≈ 0.6400

The PCR model achieves cross-validated predictive performance comparable to the full linear regression model, but substantially lower than that of the reduced linear model.

While PCR effectively addresses multicollinearity through the use of orthogonal components, this dimensionality reduction does not translate into improved out-of-sample performance for this dataset. This suggests that the variance captured by the retained principal components is not fully aligned with the variance most relevant for predicting the response.

### Conclusion

Principal Component Regression was explored as a method to reduce dimensionality and address multicollinearity among predictors. While PCR improves coefficient stability by using orthogonal components, cross-validation indicates no meaningful improvement in predictive performance relative to the full linear regression model.

This suggests that the directions of greatest predictor variance are not strongly aligned with the variation most relevant for predicting crime rates. Given the limited sample size and the importance of interpretability, PCR is best viewed here as a robustness check rather than a primary modeling approach.