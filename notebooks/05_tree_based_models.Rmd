---
title: "Tree-Based Models"
author: "Catalina Valencia"
output: html_document
---

**Project:** US Crime Rate Modeling: Comparing Regression, Regularization, and Tree-Based Methods

**File:** 05_tree_based_models.Rmd

### Objectives:
- Explore non-linear and interaction effects in US crime data.
- Fit and evaluate regression tree models.
- Fit and evaluate random forest models.
- Compare predictive performance and key variable importance.

### Notes:
- Regression trees provide interpretable branching rules but can overfit.
- Pruning mitigates overfitting and improves generalization.
- Random Forest aggregates multiple trees to stabilize predictions and reduce variance.
- Emphasis is on predictive accuracy and qualitative insights, not formal inference.

Environment Setup

```{r}
rm(list = ls())
set.seed(3)
```

Load required libraries

```{r}
library(tree)
library(randomForest)
```

Load Data

```{r}
uscrimedata <- read.table(
  "../data/uscrime.txt",
  header = TRUE,
  stringsAsFactors = FALSE
)
```

Split Data into Training and Test Sets

```{r}
n <- nrow(uscrimedata)
train_idx <- sample(1:n, size = 0.7 * n)
train <- uscrimedata[train_idx, ]
test  <- uscrimedata[-train_idx, ]
```

### Regression Tree Model

Fit regression tree on training data

```{r}
tree_model <- tree(Crime ~ ., data = train)
summary(tree_model)
```

Plot tree

```{r}
if (nrow(tree_model$frame) > 1) {
  plot(tree_model)
  text(tree_model, cex = 0.7)
}
```

**Cross-validation to determine optimal tree size**

```{r}
cv_tree <- cv.tree(tree_model)
plot(cv_tree$size, cv_tree$dev, type = "b",
     xlab = "Tree Size", ylab = "Deviance",
     main = "CV for Tree Size")

best_nodes <- cv_tree$size[which.min(cv_tree$dev)]
best_nodes
```

Prune tree to optimal size

```{r}
pruned_tree <- prune.tree(tree_model, best = best_nodes)
summary(pruned_tree)
```

Plot tree

```{r}
plot(tree_model)
text(tree_model, cex = 0.7)
```

Predictions on test set
```{r}
yhat_tree <- predict(tree_model, newdata = test)
if (nrow(pruned_tree$frame) > 1) {
  yhat_pruned <- predict(pruned_tree, newdata = test)
  rmse_pruned_tree <- sqrt(mean((test$Crime - yhat_pruned)^2))
} else {
  yhat_pruned <- rep(mean(train$Crime), nrow(test))
  rmse_pruned_tree <- sqrt(mean((test$Crime - yhat_pruned)^2))
}
```

Compute RMSE

```{r}
rmse_tree <- sqrt(mean((test$Crime - yhat_tree)^2))
rmse_pruned_tree <- sqrt(mean((test$Crime - yhat_pruned)^2))
```

**Results:**

The regression tree model identifies key predictors: Po1, Po2, and M.

Cross-validation selects a tree with 4 terminal nodes.

Test RMSE shows slight improvement after pruning:

Full tree: 374.698

Pruned tree: 371.086

- Tree captures non-linear interactions among predictors.

- Pruning reduces overfitting while maintaining predictive accuracy.

- Key insight: lower Po1 values are associated with lower predicted crime rates.

### Random Forest Model

Fit random forest model on training data

```{r}
rf_model <- randomForest(Crime ~ ., data = train, mtry = 4, importance = TRUE)
rf_model
```

Predictions on test set

```{r}
yhat_rf <- predict(rf_model, newdata = test)
rmse_rf <- sqrt(mean((test$Crime - yhat_rf)^2))
```

Results:

Random Forest model details:

Number of trees: 500

Number of predictors tried at each split (mtry): 4

Training set % variance explained: 36.47%

Test RMSE: 354.663

- Random Forest outperforms a single regression tree (lower RMSE).

- Key predictors (IncNodePurity, %IncMSE): Po1, Po2, Ineq, Prob.

- Handles multicollinearity and overfitting effectively.

- Cities with lower Po1 (<7.65) are predicted more accurately than cities with higher Po1.

Visualize variable importance

```{r}
varImpPlot(rf_model)
```

### Conclusion

Tree-based models were used to capture non-linear relationships in the US crime data. A single regression tree offered interpretability but limited predictive accuracy, even after pruning.

The random forest model improved test-set performance by reducing variance through ensemble averaging and consistently identified Po1 and Po2 as the most influential predictors.

Overall, tree-based methods enhance predictive performance and reveal complex structure but are better suited for prediction than formal inference in this study.